import os
import scipy.io
import numpy as np
import csv
import pickle
import torch
from test_code import filter_checkpoint_weights
# from model.speech_dw_former import SpeechDW_Former
import json
from loguru import logger
import matplotlib.pyplot as plt
import matplotlib.cm as cm
# from sklearn import preprocessing

hubert_feature_path = "/content/drive/MyDrive/speechformer2/iemocap/feature/hubert_large_L12_mat"
metadata_path = "/content/drive/MyDrive/speechformer2/metadata/metadata_iemocap.csv"

checkpoint_file = "/content/drive/MyDrive/speechformer2/exp/SpeechDW/iemocap_e120_b32_lr0.001_hubert12_04-06-2024 02-28-01_/fold_1/checkpoint/epoch_19_68090.pt"
model_config_file = "/content/drive/MyDrive/speechformer2/config/model_config.json"
feature_config_file = "/content/drive/MyDrive/speechformer2/config/iemocap_feature_config.json"
conveters = {
        'iemocap': {'ang': 0, 'neu': 1, 'hap': 2, 'exc': 2, 'sad': 3},
        'meld': {'neutral': 0, 'anger': 1, 'joy': 2, 'sadness': 3, 'surprise': 4, 'disgust': 5, 'fear': 6},
        'pitt': {'Control': 0, 'Dementia': 1},
        'daic_woz': {'not-depressed': 0, 'depressed': 1}
    }
index_to_unit_level = {"0": "FRAME", "1": "PHONE", "2": "WORD", "3": "UTTERANCE"}

def load_pretrained_extractor():
    model_type = "SpeechDW"
    dataset = "iemocap"
    feature = "hubert12"
    
    with open(model_config_file, 'r') as f1, open(feature_config_file, 'r') as f2:
        model_json = json.load(f1)[model_type]
        feas_json = json.load(f2)
        data_json = feas_json[feature]
        data_json['meta_csv_file'] = feas_json['meta_csv_file']
        model_json['num_classes'] = feas_json['num_classes']
        model_json['input_dim'] = (data_json['feature_dim'] // model_json['num_heads']) * model_json['num_heads']
        model_json['length'] = data_json['length']
        model_json['ffn_embed_dim'] = model_json['input_dim'] // 2
        model_json['hop'] = data_json['hop']

    extractor = SpeechDW_Former(**model_json).to('cuda')
    ckpt = torch.load(checkpoint_file)
    
    # filtered_state_dict = filter_checkpoint_weights(ckpt['model'])
    extractor.load_state_dict(ckpt['model'])
    
    return extractor

def plot_bar_chart(data):
    fig, axes = plt.subplots(nrows=len(data), ncols=1, figsize=(12, 8))

    for i, (values, duration, color_array) in enumerate(data):
        num_values = len(values)
        x = np.arange(num_values) * duration
        
        # Create a colormap
        cmap = cm.get_cmap('viridis', max(color_array) + 1)  # Add 1 to include all colors
        colors = [cmap(c) for c in color_array]  # Map each number to a color from the colormap
        
        # Iterate through colors and values to draw bars and vertical lines
        for j, (value, color) in enumerate(zip(values, colors)):
            axes[i].bar(x[j], value, width=duration, align='edge', color=color)
            if j < len(colors) - 1 and colors[j] != colors[j + 1]:
                # Draw vertical line when color changes
                axes[i].axvline(x=(j + 1) * duration, color='red', linestyle='--', linewidth=1)
        
        axes[i].set_xlabel('Time (seconds)')
        axes[i].set_ylabel('Importance score')
        axes[i].set_title(f'{index_to_unit_level[str(i)]} STAGE')
        
        axes[i].set_xlim(0, 7)

    plt.tight_layout()
    plt.show()

def normalize_min_max(data):
    data = np.asarray(data)
    min = np.min(data)
    max = np.max(data)
    return (data - min) / (max - min)

def normalize_tensor(tensor):
    # Calculate minimum and maximum values along dimension 1
    min_vals, _ = torch.min(tensor, dim=1, keepdim=True)
    max_vals, _ = torch.max(tensor, dim=1, keepdim=True)
    
    # Normalize each row of the tensor
    normalized_tensor = (tensor - min_vals) / (max_vals - min_vals + 1e-8)  # Add a small value to avoid division by zero
    
    return normalized_tensor

def weights_to_color_array(weights, threshold=0.5) -> list:
    # weight to color array
    num_frames = weights.size(dim=0)
    window_mapping = torch.zeros(num_frames, dtype=torch.int)
    weights = torch.ge(weights, threshold)
    
    frame_start = 0
    color_value = 0
    
    for frame_cnt in range(1, num_frames):
        # same window
        if not (weights[frame_cnt] ^ weights[frame_start]):
            continue
        # current frame is begin of new window => check the window size = frame_cnt - frame_start
        if frame_start + 1 == frame_cnt:
            weights[frame_start] = weights[frame_cnt]
            continue
        
        window_mapping[frame_start: frame_cnt] = color_value
        color_value += 1
        
        frame_start = frame_cnt
    
    frame_cnt += 1
    window_mapping[frame_start: frame_cnt] = color_value
    
    frame_start = frame_cnt
    return window_mapping

def weights_to_color_arrays(weights) -> dict:
    color_arrays = {}
    for key in weights.keys():
        color_arrays[key] = weights_to_color_array(weights[key])
        print(color_arrays[key])
    return color_arrays

def main():
    durations = {
        "frame": 0.02,
        "phone": 0.05,
        "word":  0.25,
        "utterance": 1
    }
    # # Ses05M_script01_1_F000    sad     good
    # weights = {
    #     "frame": torch.tensor(
    #         [0.7373, 0.4977, 0.4165, 0.4108, 0.3687, 0.4791, 0.5297, 0.5558, 0.3482,
    #      0.2056, 0.2417, 0.2803, 0.4447, 0.3290, 0.3189, 0.1575, 0.0796, 0.2785,
    #      0.1903, 0.2165, 0.1693, 0.2638, 0.3065, 0.2660, 0.1572, 0.1457, 0.5030,
    #      0.3675, 0.3185, 0.2139, 0.1328, 0.0119, 0.1850, 0.3758, 0.3560, 0.4889,
    #      0.2969, 0.4170, 0.5624, 0.6060, 0.6825, 0.5898, 0.7301, 0.5613, 0.2869,
    #      0.1380, 0.3196, 0.5252, 0.4992, 0.6549, 0.7498, 0.7802, 0.8056, 0.8316,
    #      0.5888, 0.4360, 0.1826, 0.5088, 0.2019, 0.3230, 0.4252, 0.5503, 0.4934,
    #      0.6036, 0.7084, 0.6439, 0.7944, 0.7499, 0.8846, 0.8924, 0.7894, 0.4983,
    #      0.6213, 0.6994, 0.6988, 0.6123, 0.8035, 0.8637, 0.7871, 0.7909, 0.4780,
    #      0.3574, 0.1715, 0.1571, 0.2697, 0.2970, 0.3238, 0.1992, 0.2446, 0.4257,
    #      0.6289, 0.6042, 0.8065, 0.8811, 0.9009, 0.8740, 0.8262, 0.7297, 0.6603,
    #      0.6054, 0.6419, 0.4694, 0.5477, 0.4845, 0.6165, 0.8601, 0.7054, 0.4982,
    #      0.4780, 0.3631, 0.3643, 0.2300, 0.4259, 0.4195, 0.2693, 0.2719, 0.2293,
    #      0.1593, 0.1765, 0.3069, 0.5350, 0.4993, 0.5248, 0.4850, 0.5467, 0.5802,
    #      0.6799, 0.6004, 0.6373, 0.5133, 0.4157, 0.4433, 0.6367, 0.6899, 0.5767,
    #      0.2939, 0.2494, 0.4388, 0.4623, 0.3521, 0.6393, 0.5481, 0.5151, 0.5783,
    #      0.5921, 0.4742, 0.4234, 0.4466, 0.4249, 0.3162, 0.4431, 0.4993, 0.5185,
    #      0.6675, 0.6701, 0.6210, 0.7087, 0.7355, 0.8454, 0.8917, 0.8988, 0.9565,
    #      0.7331, 0.3964, 0.1355, 0.1919, 0.3279, 0.4213, 0.4474, 0.5557, 0.4677,
    #      0.5525, 0.7120, 1.0000, 0.9373, 0.8020, 0.6880, 0.6720, 0.4928, 0.3078,
    #      0.4703, 0.4893, 0.3439, 0.1454, 0.0000, 0.3065, 0.4370, 0.2892, 0.6046,
    #      0.5640, 0.3383, 0.6778, 0.5830, 0.5916, 0.4059, 0.2658, 0.3586, 0.4088,
    #      0.3726, 0.3462, 0.4638, 0.3550, 0.3919, 0.3866, 0.4423, 0.4206, 0.4251,
    #      0.4497, 0.4841, 0.5607, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133, 0.8133,
    #      0.8133, 0.8133]
    #         ),
    #     "phone": torch.tensor(
    #         [0.5303, 0.5203, 0.5593, 0.6162, 0.6385, 0.7319, 0.6558, 0.5977, 0.6393,
    #      0.5035, 0.5407, 0.5644, 0.5297, 0.6203, 0.5178, 0.5529, 0.4673, 0.6940,
    #      0.6681, 0.5936, 0.6148, 0.5463, 0.6913, 0.9140, 0.6988, 0.8635, 0.7355,
    #      0.8117, 0.8711, 0.6499, 0.3028, 0.3539, 0.3393, 0.5100, 0.8169, 0.7550,
    #      0.7773, 0.6656, 0.7987, 0.9118, 0.6012, 0.6196, 0.5786, 0.5839, 0.7437,
    #      0.8222, 0.5882, 0.4099, 0.5260, 0.7406, 0.8260, 0.9510, 0.7916, 0.7497,
    #      0.8853, 0.7899, 0.7973, 0.9166, 0.8487, 0.8727, 0.8903, 0.6236, 0.4679,
    #      0.3526, 0.4296, 0.5618, 0.5249, 0.9242, 0.9901, 1.0000, 0.7845, 0.8192,
    #      0.6577, 0.6470, 0.8411, 0.9038, 0.9333, 0.5791, 0.5423, 0.5804, 0.7745,
    #      0.8678, 0.8410, 0.7744, 0.7610, 0.7480, 0.8512, 0.8240, 0.7446, 0.7544,
    #      0.8214, 0.9466, 0.8417, 0.8446, 0.9270, 0.8368, 0.8332, 0.8164, 0.7895,
    #      0.7030, 0.6273, 0.6046, 0.5197, 0.4685, 0.4265, 0.1521, 0.0158, 0.0158,
    #      0.0158, 0.0158, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,
    #      0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,
    #      0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,
    #      0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,
    #      0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155,
    #      0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0156, 0.0112, 0.0112,
    #      0.0000]
    #         ),
    #     "word": torch.tensor(
    #         [0.8596, 0.9861, 0.7497, 0.7637, 0.7247, 0.8712, 0.9176, 1.0000, 0.9589,
    #      0.8814, 0.6998, 0.6709, 0.6566, 0.7264, 0.8592, 0.8460, 0.7299, 0.1545,
    #      0.0069, 0.0041, 0.0041, 0.0041, 0.0041, 0.0041, 0.0043, 0.0043, 0.0000,
    #      0.0141]
    #     ),
    #     "utterance": torch.tensor(
    #         [1.0000, 0.9140, 0.9514, 0.8990, 0.1206, 0.0417, 0.0000]
    #     )
    #     }
    
    # # Ses05F_impro05_F020:    angry    bad
    # weights = {
    #     "frame": torch.tensor(
    #         [0.7226, 0.6963, 0.6738, 0.6560, 0.5832, 0.4559, 0.4471, 0.5237, 0.4935,
    #      0.4243, 0.4096, 0.4864, 0.5091, 0.4172, 0.4165, 0.4362, 0.4706, 0.5349,
    #      0.5545, 0.5042, 0.5183, 0.4758, 0.5192, 0.3719, 0.5265, 0.4921, 0.4970,
    #      0.2915, 0.5045, 0.4537, 0.5740, 0.5631, 0.4938, 0.4680, 0.5159, 0.4542,
    #      0.4408, 0.4456, 0.3643, 0.3129, 0.3154, 0.3355, 0.3030, 0.3290, 0.3514,
    #      0.2725, 0.3817, 0.3319, 0.3328, 0.3681, 0.5084, 0.7283, 0.7357, 0.7739,
    #      0.7508, 0.7348, 0.6726, 0.5789, 0.5690, 0.4515, 0.2848, 0.2197, 0.2757,
    #      0.2437, 0.1136, 0.1352, 0.2565, 0.3406, 0.0706, 0.1462, 0.3533, 0.5266,
    #      0.5556, 0.5865, 0.5206, 0.5404, 0.6474, 0.8689, 1.0000, 0.8749, 0.7601,
    #      0.6003, 0.5137, 0.3842, 0.7189, 0.6288, 0.6364, 0.6623, 0.8470, 0.8513,
    #      0.8647, 0.7135, 0.7447, 0.5876, 0.4325, 0.3079, 0.3740, 0.4290, 0.6389,
    #      0.5391, 0.5228, 0.6161, 0.6909, 0.6705, 0.6369, 0.6503, 0.6837, 0.4621,
    #      0.5105, 0.4901, 0.5087, 0.6026, 0.6036, 0.6334, 0.5012, 0.4612, 0.5070,
    #      0.4460, 0.5173, 0.4839, 0.3652, 0.3074, 0.3434, 0.2754, 0.3532, 0.2731,
    #      0.4187, 0.3649, 0.3776, 0.6064, 0.4735, 0.3946, 0.3601, 0.3694, 0.3077,
    #      0.2685, 0.4479, 0.3625, 0.2488, 0.2497, 0.3661, 0.5316, 0.4590, 0.3765,
    #      0.4728, 0.4529, 0.3885, 0.3555, 0.4250, 0.6361, 0.5538, 0.4918, 0.3428,
    #      0.2208, 0.2941, 0.3272, 0.1672, 0.0787, 0.2016, 0.2920, 0.4060, 0.3149,
    #      0.1511, 0.4603, 0.4538, 0.3372, 0.3317, 0.4583, 0.4872, 0.5829, 0.4201,
    #      0.5198, 0.5759, 0.5027, 0.2849, 0.4925, 0.6090, 0.4299, 0.2084, 0.4884,
    #      0.5819, 0.6292, 0.7089, 0.6712, 0.6406, 0.5487, 0.5233, 0.5124, 0.4726,
    #      0.6170, 0.6410, 0.6875, 0.6948, 0.4780, 0.3883, 0.3657, 0.6174, 0.6615,
    #      0.6698, 0.6724, 0.4174, 0.5812, 0.5763, 0.6188, 0.7152, 0.5270, 0.1154,
    #      0.3521, 0.4845, 0.4155, 0.4838, 0.6138, 0.7715, 0.7183, 0.5352, 0.5167,
    #      0.4709, 0.7651, 0.5602, 0.5407, 0.5652, 0.4829, 0.3016, 0.1325, 0.0500,
    #      0.2336, 0.5638, 0.4833, 0.3664, 0.5265, 0.5253, 0.2391, 0.1730, 0.0000,
    #      0.1578, 0.3635, 0.4009, 0.1184, 0.3884, 0.3933, 0.3227, 0.4027, 0.4028,
    #      0.5184, 0.5842, 0.5821, 0.5145, 0.6931, 0.7433, 0.6647, 0.4462, 0.2983,
    #      0.5014, 0.4040, 0.4926, 0.3196, 0.5407, 0.4684, 0.4236, 0.3262, 0.4726,
    #      0.4764, 0.5159, 0.5541, 0.4312, 0.4476, 0.6001, 0.7134, 0.6165, 0.5385,
    #      0.5207, 0.3168, 0.2659, 0.1847, 0.1119, 0.1250, 0.2400, 0.3849, 0.5459,
    #      0.4197, 0.5418, 0.6648, 0.6611, 0.5039, 0.6646, 0.3194, 0.3713, 0.4049,
    #      0.5322, 0.6339, 0.7374, 0.7076, 0.7027, 0.6082, 0.5475, 0.3596, 0.1192,
    #      0.1256, 0.2017, 0.1282, 0.0265, 0.1365, 0.3516, 0.4401, 0.3652, 0.4763,
    #      0.3786, 0.4375, 0.4988, 0.6353, 0.7412, 0.7689, 0.6827, 0.5727, 0.5531,
    #      0.7914, 0.6420, 0.6429, 0.6577, 0.6083, 0.5519, 0.5588, 0.5275, 0.5367,
    #      0.3864, 0.3695]
    #         ),
    #     "phone": torch.tensor(
    #         [0.3423, 0.3478, 0.2108, 0.2466, 0.2023, 0.1243, 0.1422, 0.1570, 0.2163,
    #      0.1931, 0.1624, 0.0743, 0.2132, 0.0595, 0.1740, 0.2076, 0.1150, 0.0750,
    #      0.1071, 0.0250, 0.0000, 0.0923, 0.1313, 0.3145, 0.4034, 0.2336, 0.2032,
    #      0.2568, 0.2216, 0.2657, 0.3400, 0.3117, 0.3897, 0.6132, 0.5242, 0.5978,
    #      0.7118, 0.6174, 0.5870, 0.8275, 0.8210, 0.8022, 0.7217, 0.6238, 0.6319,
    #      0.6105, 0.6015, 0.3734, 0.1984, 0.5329, 0.6096, 0.2969, 0.2718, 0.4977,
    #      0.9041, 0.8935, 0.5908, 0.4562, 0.3469, 0.4203, 0.4679, 0.4110, 0.4318,
    #      0.3844, 0.3848, 0.4786, 0.5074, 0.4148, 0.4807, 0.7609, 0.5540, 0.5367,
    #      0.5117, 0.4224, 0.5933, 0.6193, 0.5636, 0.6985, 0.6944, 0.5381, 0.5181,
    #      0.4032, 0.3885, 0.2922, 0.3650, 0.3192, 0.2863, 0.4548, 0.8427, 0.6213,
    #      0.5812, 0.6673, 0.5681, 0.3284, 0.2857, 0.4857, 0.5100, 0.4576, 0.6041,
    #      0.5200, 0.4910, 0.6565, 0.4491, 0.5432, 0.6306, 0.3051, 0.4108, 0.5241,
    #      0.5024, 0.5537, 0.5731, 0.5587, 0.5039, 0.5948, 0.5830, 0.5695, 0.5479,
    #      0.6800, 0.4771, 0.2374, 0.4039, 0.4053, 0.6312, 0.5517, 0.6109, 0.7667,
    #      0.7329, 0.5492, 0.4524, 0.3618, 0.4314, 0.4490, 0.4561, 0.4146, 0.4871,
    #      0.3873, 0.4221, 0.7201, 0.7822, 0.4727, 0.6763, 0.6711, 0.6458, 0.6359,
    #      0.4551, 0.3248, 0.1567, 0.3030, 0.5245, 0.5199, 0.5201, 0.5175, 0.5699,
    #      0.7317, 1.0000, 0.7517, 0.5672, 0.1566, 0.2371, 0.4326, 0.3768, 0.2766,
    #      0.3182]
    #         ),
    #     "word": torch.tensor(
    #         [0.9570, 1.0000, 0.9533, 0.3824, 0.0000, 0.3594, 0.5225, 0.4710, 0.8204,
    #      0.7984, 0.6004, 0.4164, 0.3467, 0.2658, 0.2350, 0.5868, 0.5663, 0.3476,
    #      0.5954, 0.4772, 0.5735, 0.4635, 0.2396, 0.5483, 0.5096, 0.4196, 0.0777,
    #      0.2650]
    #     ),
    #     "utterance": torch.tensor(
    #         [0.3795, 0.0000, 0.9015, 0.3014, 0.9875, 0.9395, 1.0000]
    #     )
    #     }
    
    # # Ses05M_impro07_F004   exc     good
    # weights = {
    #     "frame": torch.tensor(
    #         [0.2819, 0.3779, 0.3306, 0.3069, 0.2957, 0.2596, 0.2294, 0.1622, 0.1454,
    #      0.0937, 0.0702, 0.0181, 0.0339, 0.2071, 0.4649, 0.2049, 0.0000, 0.1281,
    #      0.2524, 0.1665, 0.4015, 0.4650, 0.3772, 0.4138, 0.3020, 0.2650, 0.5179,
    #      0.5571, 0.5983, 0.6401, 0.9746, 1.0000, 0.5134, 0.2216, 0.4092, 0.5112,
    #      0.6545, 0.5104, 0.2340, 0.0238, 0.0911, 0.1131, 0.1005, 0.1222, 0.2925,
    #      0.3100, 0.1721, 0.2837, 0.2185, 0.1795, 0.3053, 0.4306, 0.4142, 0.5772,
    #      0.6264, 0.7740, 0.6217, 0.5328, 0.4875, 0.2928, 0.2438, 0.0022, 0.0563,
    #      0.1693, 0.2527, 0.0145, 0.0993, 0.2214, 0.1173, 0.2425, 0.2562, 0.1642,
    #      0.2544, 0.3278, 0.3910, 0.2713, 0.2858, 0.1953, 0.2262, 0.1863, 0.0592,
    #      0.0972, 0.1656, 0.1768, 0.0681, 0.1689, 0.1263, 0.1063, 0.2074, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214, 0.5214,
    #      0.5214, 0.5214]
    #         ),
    #     "phone": torch.tensor(
    #         [0.5097, 0.5057, 0.5019, 0.3335, 0.1800, 0.2625, 0.1399, 0.0000, 0.1962,
    #      0.5042, 0.4905, 0.4218, 0.4627, 1.0000, 0.9203, 0.9261, 0.6390, 0.7965,
    #      0.3906, 0.5620, 0.8240, 0.7750, 0.6395, 0.6357, 0.0383, 0.0285, 0.1264,
    #      0.3771, 0.5331, 0.6150, 0.8239, 0.8033, 0.3812, 0.3864, 0.4654, 0.4781,
    #      0.4356, 0.4258, 0.5733, 0.6835, 0.7509, 0.6747, 0.7663, 0.6741, 0.6484,
    #      0.0868, 0.0671, 0.0671, 0.0671, 0.0671, 0.0671, 0.0671, 0.0671, 0.0671,
    #      0.0671, 0.0671, 0.0671, 0.0671, 0.0671, 0.0671, 0.0671, 0.0672, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673, 0.0673,
    #      0.0436]
    #         ),
    #     "word": torch.tensor(
    #         [0.7557, 0.8675, 0.8473, 0.9765, 0.9812, 1.0000, 0.8709, 0.1634, 0.0079,
    #      0.0106, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,
    #      0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0106, 0.0100, 0.0000,
    #      0.0160]
    #     ),
    #     "utterance": torch.tensor(
    #         [1.0000, 0.7903, 0.0191, 0.0554, 0.0520, 0.0528, 0.0000]
    #     )
    #     }
    
    # Ses05F_impro05_F000   neu     x
    weights = {
        "frame": torch.tensor(
            [0.6547, 0.5317, 0.4837, 0.4269, 0.6309, 0.5376, 0.5068, 0.2593, 0.3828,
         0.4426, 0.5715, 0.4955, 0.4458, 0.4398, 0.4167, 0.3923, 0.3923, 0.2673,
         0.3985, 0.5205, 0.3994, 0.4601, 0.2948, 0.4714, 0.3960, 0.5316, 0.4773,
         0.4236, 0.3997, 0.5112, 0.4650, 0.4774, 0.3163, 0.3793, 0.3166, 0.3918,
         0.3881, 0.3721, 0.3333, 0.3270, 0.4620, 0.3996, 0.4374, 0.4801, 0.6083,
         0.6306, 0.7761, 0.8359, 0.8570, 0.8109, 0.7960, 0.8194, 0.7824, 0.8418,
         0.8067, 0.7852, 0.8046, 0.7269, 0.7200, 0.6449, 0.5640, 0.4921, 0.3666,
         0.2907, 0.2992, 0.2669, 0.3262, 0.3761, 0.3533, 0.3273, 0.2883, 0.4876,
         0.4166, 0.3676, 0.2323, 0.6348, 0.6836, 0.7300, 0.6140, 0.7258, 0.7939,
         0.8957, 0.9093, 0.9035, 0.8643, 0.8454, 0.8450, 0.8921, 0.8798, 0.7910,
         0.7394, 0.6698, 0.7054, 0.6718, 0.6217, 0.5794, 0.5231, 0.6465, 0.6940,
         0.7140, 0.7022, 0.5895, 0.6539, 0.6592, 0.3649, 0.6148, 0.5469, 0.5871,
         0.5081, 0.4689, 0.3683, 0.5103, 0.5353, 0.5371, 0.4596, 0.4594, 0.3962,
         0.4533, 0.5766, 0.5938, 0.6774, 0.5886, 0.5176, 0.4323, 0.4683, 0.5917,
         0.5069, 0.5534, 0.5117, 0.5458, 0.5252, 0.4986, 0.4662, 0.4702, 0.3863,
         0.3933, 0.3752, 0.4035, 0.2062, 0.2759, 0.2727, 0.3410, 0.2628, 0.3069,
         0.5038, 0.3215, 0.4272, 0.3802, 0.1664, 0.3725, 0.6237, 0.7755, 0.7057,
         0.5739, 0.5873, 0.5318, 0.4976, 0.5486, 0.6331, 0.7068, 0.6646, 0.6125,
         0.3789, 0.4829, 0.5987, 0.3809, 0.2769, 0.3642, 0.6183, 0.7681, 0.7188,
         0.6199, 0.5678, 0.3744, 0.2979, 0.2874, 0.3965, 0.6590, 0.7337, 0.7649,
         0.6364, 0.7573, 0.8334, 1.0000, 0.9206, 0.8122, 0.6110, 0.4765, 0.6283,
         0.7626, 0.7658, 0.6764, 0.5452, 0.4305, 0.4498, 0.5186, 0.5909, 0.4892,
         0.4909, 0.4777, 0.4248, 0.2091, 0.1148, 0.2055, 0.0033, 0.0000, 0.0295,
         0.2158, 0.3500, 0.5894, 0.6724, 0.6756, 0.7643, 0.6236, 0.5874, 0.5530,
         0.5943, 0.5644, 0.6130, 0.5824, 0.6141, 0.4661, 0.4570, 0.5378, 0.5003,
         0.5493, 0.3346, 0.2464, 0.2001, 0.2822, 0.3045, 0.4518, 0.5236, 0.5739,
         0.5787, 0.5989, 0.4622, 0.5518, 0.6380, 0.7299, 0.6492, 0.6420, 0.6253,
         0.5020, 0.5775, 0.6017, 0.6063, 0.5708, 0.6148, 0.6866, 0.7072, 0.4927,
         0.4790, 0.5586, 0.5484, 0.5561, 0.5588, 0.6875, 0.5672, 0.5052, 0.5642,
         0.5123, 0.5370, 0.6377, 0.7093, 0.6888, 0.6777, 0.6572, 0.6754, 0.6722,
         0.6896, 0.7523, 0.7346, 0.6996, 0.6013, 0.5837, 0.7838, 0.7003, 0.8215,
         0.7599, 0.5420, 0.4035, 0.4388, 0.6646, 0.7232, 0.7618, 0.7708, 0.8324,
         0.8791, 0.8145, 0.6729, 0.6406, 0.6440, 0.5975, 0.5389, 0.5602, 0.5359,
         0.5696, 0.6179, 0.5035, 0.5499, 0.5399, 0.5351, 0.5015, 0.5058, 0.4910,
         0.5119, 0.5365, 0.3654, 0.5063, 0.3442, 0.4651, 0.3957, 0.4281, 0.4677,
         0.4819, 0.6059, 0.6425, 0.5551, 0.5651, 0.5365, 0.5620, 0.4911, 0.4913,
         0.4129, 0.3853]
            ),
        "phone": torch.tensor(
            [0.6540, 0.6124, 0.5841, 0.6985, 0.7643, 0.5904, 0.8126, 0.7881, 0.7673,
         0.6650, 0.8563, 0.7841, 0.5656, 0.5425, 0.5297, 0.1908, 0.3912, 0.4472,
         0.0484, 0.4524, 0.5153, 0.6048, 0.6857, 0.5155, 0.4733, 0.4631, 0.4688,
         0.3307, 0.2310, 0.1981, 0.2652, 0.3810, 0.1251, 0.1100, 0.3050, 0.2084,
         0.3215, 0.3079, 0.2978, 0.0983, 0.0000, 0.1144, 0.1437, 0.1976, 0.1051,
         0.1426, 0.2566, 0.3198, 0.3415, 0.3701, 0.5010, 0.5300, 0.5512, 0.6757,
         0.6833, 0.7085, 0.3518, 0.5785, 0.7544, 0.5411, 0.6422, 0.5946, 0.5952,
         0.5668, 0.4500, 0.5738, 0.5362, 0.2235, 0.1274, 0.1837, 0.1199, 0.4810,
         0.2696, 0.4205, 0.6819, 0.1660, 0.1383, 0.4973, 0.5223, 0.4791, 0.5586,
         0.2114, 0.4909, 0.7861, 0.6332, 0.7155, 0.7362, 0.5933, 0.7623, 0.6527,
         0.3766, 0.3427, 0.3915, 0.4969, 0.5453, 0.4372, 0.7272, 0.5483, 0.4329,
         0.7533, 0.8644, 0.5920, 0.7424, 0.7841, 0.6831, 0.6262, 0.3640, 0.3272,
         0.5302, 0.6617, 0.5252, 0.3216, 0.5118, 0.9265, 0.5198, 0.6107, 0.4324,
         0.4295, 0.4556, 0.7643, 0.7630, 0.4244, 0.4808, 0.7959, 0.7590, 1.0000,
         0.9383, 0.7301, 0.5992, 0.6539, 0.6294, 0.2858, 0.4159, 0.4304, 0.4238,
         0.3070, 0.3107, 0.6175, 0.3849, 0.3606, 0.4577, 0.5392, 0.7257, 0.5758,
         0.6054, 0.3962, 0.3327, 0.4315, 0.7316, 0.8183, 0.6645, 0.5405, 0.4223,
         0.5936, 0.6621, 0.6381, 0.7465, 0.7411, 0.6741, 0.6969, 0.7474, 0.7276,
         0.5932]
            ),
        "word": torch.tensor(
            [0.6986, 0.9180, 0.7991, 0.5278, 0.6949, 0.6528, 0.4445, 0.5003, 0.8235,
         0.9285, 1.0000, 0.9626, 0.8038, 0.6141, 0.0023, 0.4047, 0.8543, 0.8003,
         0.3833, 0.0649, 0.2810, 0.6757, 0.2510, 0.0000, 0.1092, 0.5395, 0.7377,
         0.7475]
        ),
        "utterance": torch.tensor(
            [0.8703, 0.6559, 0.7413, 1.0000, 0.8096, 0.5604, 0.0000]
        )
        }
    
    color_arrays = weights_to_color_arrays(weights)
    data = [
        (weights['frame'], durations['frame'], color_arrays['frame']),  # First chart
        (weights['phone'], durations['phone'], color_arrays['phone']),  # First chart
        (weights['word'], durations['word'], color_arrays['word']),  # First chart
        (weights['utterance'], durations['utterance'], color_arrays['utterance']),     # Second chart
        # ([7, 4, 9, 3, 5], 3, [0, 1, 2, 3, 4])      # Third chart
    ]
    plot_bar_chart(data)
    
  
    # extractor = load_pretrained_extractor()
    
    # from utils.dataset_lmdb import LMDB_Dataset

    # dataset = LMDB_Dataset(
    #   corpus="iemocap",
    #   lmdb_root="/content/drive/MyDrive/speechformer2/lmdb/iemocap_hubert_L12", 
    #   map_size=326 * 1024 * 4, 
    #   label_conveter = conveters['iemocap'], 
    # #   state="train", 
    #   state="test", 
    #   mode="constant", 
    #   length=326, 
    #   feature_dim=1024, 
    #   pad_value=0, 
    #   fold=1
    # )

    # print(f"Length of test dataset: {len(dataset)}")

    # extractor.eval()

    # for idx in range(len(dataset)):
    #     x, y, filename = dataset[idx]
    #     x_cuda = x.to("cuda")

    #     batch = torch.stack([x_cuda], dim = 0)
    #     with torch.no_grad():
    #         y_pred = extractor(batch)
            
    #     logger.info(f"result: {y_pred == y}, y_pred: {y_pred}, y_label: {y}, filename: {filename}")

if __name__ == "__main__":
  main()

